name: Live Data Stream (Micro-Batch)

on:
  schedule:
    # This runs the job every 5 minutes
    - cron: "*/5 * * * *"
  workflow_dispatch:

jobs:
  run-streaming-batch:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Dependencies
        run: pip install -r requirements.txt

      # --- NEW STEP: DOWNLOAD DATA FROM KAGGLE ---
      - name: Download Dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          # Ensure kaggle library is installed
          pip install kaggle
          mkdir -p ./data
          # Download only train.csv
          kaggle competitions download -c store-sales-time-series-forecasting -f train.csv -p ./data/
          # Unzip it
          unzip ./data/train.csv.zip -d ./data/
          # Remove zip file to save space
          rm ./data/train.csv.zip
          echo "âœ… Dataset downloaded successfully."

      - name: 1. Run Producer Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python producer_batch.py

      - name: 2. Run Feature Processor Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python feature_store_batch.py
