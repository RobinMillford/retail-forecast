name: Live Data Stream (Micro-Batch)

on:
  schedule:
    # This runs the job every 5 minutes
    - cron: "*/5 * * * *"
  workflow_dispatch:

jobs:
  run-streaming-batch:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: 1. Download Full Dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          pip install kaggle
          mkdir -p ./data

          # Download the ENTIRE competition archive (all csv files)
          # We removed '-f train.csv' so it downloads everything
          kaggle competitions download -c store-sales-time-series-forecasting -p ./data/

          # Unzip the main archive
          unzip -o ./data/*.zip -d ./data/

          # Clean up the zip file to save space
          rm ./data/*.zip

          echo "âœ… All datasets (Oil, Holidays, Stores, Train) extracted."
          ls -lh ./data/

      - name: 2. Run Producer Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python producer_batch.py

      - name: 3. Run Feature Processor Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python feature_store_batch.py
