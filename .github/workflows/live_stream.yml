name: Live Data Stream (Micro-Batch)

on:
  schedule:
    - cron: "*/5 * * * *"
  workflow_dispatch:

jobs:
  run-streaming-batch:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: 1. Download Dataset from Kaggle
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          pip install kaggle
          mkdir -p ./data

          # Download the specific file
          kaggle competitions download -c store-sales-time-series-forecasting -f train.csv -p ./data/

          # DEBUG: List files to see what Kaggle actually downloaded
          echo "ðŸ“‚ Files in ./data folder:"
          ls -lh ./data/

          # Robust Unzip: Find ANY .zip file and unzip it
          unzip -o ./data/*.zip -d ./data/

          # Clean up
          rm ./data/*.zip
          echo "âœ… Dataset ready."

      - name: 2. Run Producer Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python producer_batch.py

      - name: 3. Run Feature Processor Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python feature_store_batch.py
