name: Live Data Stream (Micro-Batch)

on:
  schedule:
    # This runs the job every 5 minutes
    - cron: "*/5 * * * *"
  workflow_dispatch:

jobs:
  run-streaming-batch:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: 1. Download and Fix Dataset
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          pip install kaggle
          mkdir -p ./data

          # Download the file (Kaggle sends a ZIP but names it .csv sometimes)
          kaggle competitions download -c store-sales-time-series-forecasting -f train.csv -p ./data/

          # Force rename to .zip
          mv ./data/train.csv ./data/train.zip

          # Unzip to get the REAL csv (~125MB)
          unzip -o ./data/train.zip -d ./data/

          # Clean up
          rm ./data/train.zip
          echo "âœ… Dataset extracted successfully."
          ls -lh ./data/

      - name: 2. Run Producer Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python producer_batch.py

      - name: 3. Run Feature Processor Batch
        env:
          UPSTASH_REDIS_REST_URL: ${{ secrets.UPSTASH_REDIS_REST_URL }}
          UPSTASH_REDIS_REST_TOKEN: ${{ secrets.UPSTASH_REDIS_REST_TOKEN }}
        run: python feature_store_batch.py
